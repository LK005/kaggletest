{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "abf855e94e432b7c892c873b2d76cc8e0b5f0412"
   },
   "source": [
    "# 1. 描述梯度下降，随机梯度下降和批量梯度下降的联系与区别。\n",
    "\n",
    "### 简述\n",
    "1. __梯度下降__ 在每轮迭代计算中，使用所有的样本；\n",
    "    - 优点：全局最优解；易于并行实现；\n",
    "    - 缺点：当样本数目很多时，训练过程会很慢。\n",
    "    - 从迭代的次数上来看，BGD迭代的次数相对较少。\n",
    "2. __随机梯度下降__ 在 每轮迭代中，只随机的抽一个样本；\n",
    "    - 优点：训练速度快；\n",
    "    - 缺点：准确度下降，并不是全局最优；不易于并行实现。\n",
    "    - 从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目\n",
    "3. __小批量梯度下降__ 在每轮迭代中，使用部分样本进行迭代计算。\n",
    "    - 上面两者的折衷方案，兼顾速度和准确度\n",
    "\n",
    "### 详述\n",
    "#### 1. 梯度下降\n",
    "梯度下降是批量梯度下降的简称（Batch Gradient Descent，BGD），它是梯度下降法最原始的形式，它的具体思路是：在更新每一参数时，都使用所有的样本来进行更新。  \n",
    "一般线性回归的假设函数为：\n",
    "$$ h_{\\theta}(x) = \\sum_{j=0}^n \\ \\theta_j x_j $$\n",
    "其损失函数\n",
    "$$ J_{train}(\\theta) =\n",
    "-\\frac{1}{2m} \\sum_{i=1}^m (y^{(i)} - h_{\\theta}(x^{(i)}))^2 $$\n",
    "对损失函数求导\n",
    "$$\\frac{\\partial{J(\\theta)}}{\\partial{\\theta}} \\ = -\\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - h_{\\theta}(x^{(i)}))x_j^{(i)}$$\n",
    "由于是最小化风险系数，所以按照每个参数$\\theta$ 的梯度\n",
    "\n",
    ".....\n",
    "\n",
    "> _公式打起来好累，去链接看吧_\n",
    "> - [[Machine Learning] 梯度下降法的三种形式BGD、SGD以及MBGD](https://www.cnblogs.com/maybe2030/p/5089753.html)\n",
    "> - [三种梯度下降的方式：批量梯度下降、小批量梯度下降、随机梯度下降](https://blog.csdn.net/UESTC_C2_403/article/details/74910107)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e15e907a6feb881a57d49109328f796a9fdc96f"
   },
   "source": [
    "# 2. 用sklean的线性模型完成kaggle房价预测问题。\n",
    "链接：https://www.kaggle.com/apapiu/regularized-linear-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "# 查看 input 下数据有哪些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('../input/train.csv',index_col='Id')\n",
    "print('train.shape', raw_train.shape)\n",
    "\n",
    "raw_test = pd.read_csv('../input/test.csv',index_col='Id')\n",
    "print('test.shape', raw_test.shape)\n",
    "\n",
    "# 查看train和test数据量，找到目标列 SalePrice\n",
    "print('target is', set(raw_train.columns) - set(raw_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6f0f34261d5de356fcf181ee33ef1036e03a0c3"
   },
   "outputs": [],
   "source": [
    "# 我认为训练集和预测集不应该合并\n",
    "# y_train = raw_train['SalePrice']\n",
    "# all_data = pd.concat((raw_train.drop('SalePrice', axis=1), raw_test),axis=0,sort=False).reset_index(drop=True)\n",
    "# print('all_data.shape:', all_data.shape)\n",
    "# 训练集   (1460, 81)\n",
    "# 预测数据 (1459, 80)\n",
    "# 合并    (2919, 80)\n",
    "# all_data.info()\n",
    "X_raw_train = raw_train.drop('SalePrice', axis=1)\n",
    "y_raw_train = raw_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6061c46062ea3f17dfcf44ca89c9d1c0bc76c32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_missing_ratio(data, transfer_dtypes=True):\n",
    "    # 获得数据丢失率\n",
    "    # https://github.com/echo-ray/utils/blob/master/get_missing_ratio.py\n",
    "    info = {}\n",
    "    for col in data.columns:\n",
    "        total_row = data.shape[0]\n",
    "        unique = data[col].value_counts()\n",
    "        unique_amount  = len(unique)\n",
    "        # if (data[col].dtype != 'category'):\n",
    "        # 将出现很少不同值的数字列 转换为类别列。ps：是否应该作为另一个函数？\n",
    "        if transfer_dtypes:\n",
    "            if unique_amount < 50:\n",
    "                data[col] = data[col].astype('category')\n",
    "                #data[col] = data[col].astype('str')\n",
    "        # 如果这一列有超过20个 unique 的值，那只查看前五个\n",
    "        unique_show = unique.to_dict()\n",
    "        if unique_amount > 20:\n",
    "            unique_show = unique.head(5).to_dict()#[:5]\n",
    "        unique = unique.to_dict()\n",
    "        total_amount = sum(unique.values())\n",
    "        missing_row = total_row - total_amount\n",
    "        missing_ratio = round((missing_row / total_row)*100, 2)\n",
    "        data_type = data[col].dtype\n",
    "\n",
    "        info[col] = {\n",
    "            'colume':col, 'missing_row':missing_row, 'data_type':data_type, 'unique':unique_show,\n",
    "            'unique_amount':unique_amount, 'missing_ratio':missing_ratio, \n",
    "        }\n",
    "        # print(f\"{col:15}|{missing_ratio:>5.2f}%|\", unique)\n",
    "    return pd.DataFrame(info).T.sort_values(by='missing_ratio', ascending=False)\n",
    "\n",
    "columns_info = get_missing_ratio(X_raw_train)\n",
    "#.sort_values(by='unique_amount').head(50)\n",
    "columns_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47d7341bd26f9736b8a65a923854f99d381eff67"
   },
   "outputs": [],
   "source": [
    "# categorical_columns = list(columns_info[columns_info['data_type'] == 'category'].index)\n",
    "# numeric_columns = list(set(columns_info.index) - set(categorical_columns))\n",
    "# columns_info['data_type'] != np.number 这样似乎并不对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37fd6359ea2a3368b7a5b9bdec1d44af185037da"
   },
   "outputs": [],
   "source": [
    "# 筛选字符型的列\n",
    "str_data = X_raw_train.select_dtypes(exclude='number')#(include=['category','object'])#\n",
    "categorical_colums = list(str_data.columns)\n",
    "print(f'{len(categorical_colums)} categorical_colums: \\n', categorical_colums)\n",
    "\n",
    "# 筛选数值型的列\n",
    "numeric_data = X_raw_train.select_dtypes(include='number')\n",
    "numeric_colums = list(numeric_data.columns)\n",
    "print(f'\\n {len(numeric_colums)} numeric_colums: \\n', numeric_colums)\n",
    "# num_data.describe()\n",
    "print('\\n有重复的列吗? ', 'Yes' if ((set(categorical_colums) & set(numeric_colums)) != set()) else 'No')\n",
    "print('有漏的列吗？  ', 'Yes' if (set(categorical_colums) | set(numeric_colums)) != set(X_raw_train.columns) else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d88328ad394be3f8a820ed7a34748614ef3f7e9"
   },
   "outputs": [],
   "source": [
    "# 应该只在类别列 查看差异？\n",
    "# for col in raw_test.columns:\n",
    "#     if col in ['Id', 'LotArea', 'MasVnrArea', 'BsmtFinSF1']:\n",
    "#         continue\n",
    "#     train_unique = set(raw_train[col].unique())\n",
    "#     test_unique = set(raw_test[col].unique())\n",
    "#     # 排除数值型特征，也许有更好的办法？\n",
    "#     if len(test_unique) > 100:\n",
    "#         continue\n",
    "#     if train_unique == test_unique:\n",
    "#         continue\n",
    "#     else:\n",
    "#         a = ((test_unique-train_unique) | (train_unique - test_unique))\n",
    "#         print(col, ':', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9a0e183d1cf9dbd1318282a6d507297f14e49c28"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4005013627a3cdb05fa6fb118cc992d9b4fa0c91"
   },
   "source": [
    "## 创建处理、模型 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a92ca06d46b3efd43de1b2ebeddb82ba6efff31"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__\n",
    "# 0.20 版后引入了 ColumnTransformer\n",
    "# http://scikit-learn.org/dev/modules/generated/sklearn.compose.ColumnTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ac4d616bdf49b08188977cf44177c20632daa41"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest,SelectPercentile, chi2\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LogisticRegression,BayesianRidge,ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "categorical_features = categorical_colums\n",
    "category_trans = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_features = numeric_colums\n",
    "numeric_trans = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "column_transfer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        #('category', category_trans, categorical_features),\n",
    "        ('numeric', numeric_trans, numeric_features)\n",
    "    ],\n",
    "    # remainder=MinMaxScaler()\n",
    "    # remainder='passthrough' # 保留原始数据？\n",
    "    # remainder='drop' # 丢掉原始数据？\n",
    ")\n",
    "# 测试下 column_transfer 有没有错误\n",
    "# column_transfer.fit(X_train)\n",
    "\n",
    "lgb = Pipeline([\n",
    "    ('preprocessor', column_transfer),\n",
    "    #('SelectKBest', SelectKBest(chi2, k=3)),\n",
    "    ('SVD', TruncatedSVD(n_components=15)),  # 如何拿到这15个特征的名字？ \n",
    "    #('PCA', SelectPercentile(percentile=0.8)),#PCA(n_components=10)),\n",
    "    ('LGB', LGBMRegressor()),\n",
    "    #('reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# -------------- 切分数据 -------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw_train, y_raw_train,\n",
    "    random_state=111, #shuffle=False\n",
    ")\n",
    "\n",
    "# ------------------训练-预测---------------------------\n",
    "lgb.fit(X_train, y_train)\n",
    "y_pred = lgb.predict(X_test)\n",
    "#print()\n",
    "print(\"model score: %.3f\" % lgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f35f4379b3b00aa9fb87f212c6ecaae38250d57f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 拿到 pipeline 组件的一些数据\n",
    "# SVD_ = lgb.steps[1][1]\n",
    "# vars(SVD_)\n",
    "# vars(lgb.steps[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5ca56947bbc06605544db0744ab166d13c69b9f"
   },
   "outputs": [],
   "source": [
    "param_grid={\n",
    "    #'SVD__n_components':[2, 6, 9, 12, 15, 20],\n",
    "    'LGB__num_leaves':[10, 20, 30, 40, 60, 90]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(lgb, param_grid, cv=3, return_train_score=False)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % grid_search.score(X_test, y_test))\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a07905b773a380589dfc928f61924257a09cb80c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2eaeb6f578969c3d7ba6e949e768bf0e0bdf26e9"
   },
   "source": [
    "## 尝试各种线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05ab5a7a149ad01c5cf461c0af27d0f00a3ab25f"
   },
   "source": [
    "Lasso\n",
    "LassoCV ？\n",
    "带 CV 的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e59584dac3b3d71766ac699b97288829793f973"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46cc629fb35ca70a661d77ecd634fcbccc0f6581"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d472b432161a098fad66735b7fa519ca0c7e81ea"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "column_transfer1 = column_transfer2 = ColumnTransformer(\n",
    "    [\n",
    "        ('category', OneHotEncoder(), str_colums),#CountVectorizer(analyzer=lambda x: [x]), 'city'),\n",
    "        ('num', RobustScaler(), num_colums),#CountVectorizer(), 'title')\n",
    "    ],\n",
    "    remainder=MinMaxScaler()\n",
    ")\n",
    "\n",
    "column_transfer2 = ColumnTransformer(\n",
    "    [\n",
    "        ('category', OneHotEncoder(), str_colums),#CountVectorizer(analyzer=lambda x: [x]), 'city'),\n",
    "        ('num', RobustScaler(), num_colums),#CountVectorizer(), 'title')\n",
    "    ],\n",
    "    remainder=MinMaxScaler()\n",
    ")\n",
    "column_transfer.fit_transform(all_data)\n",
    "\n",
    "\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa8de7a7df37c1b40fecf43fdd3131e8af7ab0d6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
